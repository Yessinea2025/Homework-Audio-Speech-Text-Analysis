"""
Multi-Modal Emotion Analysis for Video Files (æ”¹é€²ç‰ˆ)
åˆ†æå½±ç‰‡çš„èªéŸ³ã€æ–‡å­—ã€å’Œè¦–è¦ºæƒ…ç·’
- æ”¯æ´è‡ªè¨‚æ–‡å­—æª”æ¡ˆè¼¸å…¥
- åªåˆ†æå››ç¨®åŸºæœ¬æƒ…ç·’ï¼šå–œæ€’å“€æ¨‚ (happy, angry, sad, neutral)
"""

import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
from datetime import timedelta
import warnings
warnings.filterwarnings('ignore')

# Audio and speech processing
import librosa
import soundfile as sf
from moviepy.editor import VideoFileClip
import speech_recognition as sr

# Deep learning for emotion recognition
try:
    from deepface import DeepFace
except ImportError:
    print("DeepFace not installed. Install with: pip install deepface")

try:
    from transformers import pipeline
except ImportError:
    print("Transformers not installed. Install with: pip install transformers")


class MultiModalEmotionAnalyzer:
    """å¤šæ¨¡æ…‹æƒ…ç·’åˆ†æå™¨"""
    
    def __init__(self, video_path, output_dir="emotion_analysis_results", text_file=None, use_chinese_model=True):
        self.video_path = video_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.text_file = text_file  # æ–°å¢ï¼šè‡ªè¨‚æ–‡å­—æª”æ¡ˆè·¯å¾‘
        self.use_chinese_model = use_chinese_model  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨ä¸­æ–‡æ¨¡å‹
        
        # Store analysis results
        self.visual_emotions = []
        self.audio_emotions = []
        self.text_emotions = []
        self.timestamps = []
        
        # æ”¹ç‚ºåªæœ‰å››ç¨®åŸºæœ¬æƒ…ç·’
        self.emotion_labels = ['happy', 'angry', 'sad', 'neutral']
        
        # ä¸­æ–‡æƒ…ç·’é—œéµè©å­—å…¸ï¼ˆçµ±ä¸€å®šç¾©ï¼‰
        self.emotion_keywords = {
            'happy': {
                'keywords': [
                    # ä¸­æ–‡
                    'é–‹å¿ƒ', 'å¿«æ¨‚', 'é«˜èˆˆ', 'å–œæ­¡', 'æ„›', 'æ£’', 'å¥½', 'è®š', 'å¤ªå¥½äº†', 
                    'æˆåŠŸ', 'å®Œæˆ', 'é”æˆ', 'æ»¿æ„', 'å¹¸ç¦', 'äº«å—', 'ç¾å¥½', 'èˆˆå¥®',
                    'å“ˆå“ˆ', 'å‘µå‘µ', 'ç¬‘', '^^', 'ğŸ˜Š', 'é–‹æ‡·', 'æ„‰å¿«', 'æ­¡æ¨‚', 'æ¬£å–œ', 'å–œæ‚…',
                    'çˆ½', 'é…·', 'æ£’æ£’', 'å²å®³', 'ç‰›', 'è´Š', 'è®šè®š', 'è€¶',
                    'å¤ªæ£’', 'å®Œç¾', 'å„ªç§€', 'ç²¾å½©', 'å¾ˆå¥½', 'ä¸éŒ¯', 'æ»¿è¶³',
                    # è‹±æ–‡
                    'happy', 'joy', 'good', 'great', 'love', 'like', 'awesome', 
                    'wonderful', 'excellent', 'amazing', 'fantastic', 'glad',
                    'haha', 'lol', 'smile', 'laugh', 'yay', 'hooray'
                ],
                'weight': 1.0
            },
            'angry': {
                'keywords': [
                    # ä¸­æ–‡
                    'ç”Ÿæ°£', 'æ†¤æ€’', 'æ°£', 'è¨å­', 'ç…©', 'çˆ›', 'ç³Ÿ', 'è©²æ­»', 'å¯æƒ¡',
                    'å—ä¸äº†', 'å¿ä¸ä½', 'ç«å¤§', 'æƒ±ç«', 'æŠ“ç‹‚', 'ç™½ç—´', 'æ™ºéšœ',
                    'åƒåœ¾', 'å»¢ç‰©', 'å»æ­»', 'æ··è›‹', 'é ', 'å¹¹', 'æ“', 'ä»–åª½',
                    'ç…©æ­»', 'ç…©äºº', 'ç…©èº', 'æƒ±', 'æ€’', 'ä¸çˆ½', 'ä¸æ»¿', 'æŠ±æ€¨',
                    # è‹±æ–‡
                    'angry', 'mad', 'hate', 'annoying', 'terrible', 'awful',
                    'damn', 'shit', 'fuck', 'stupid', 'idiot', 'pissed'
                ],
                'weight': 1.2
            },
            'sad': {
                'keywords': [
                    # ä¸­æ–‡
                    'é›£é', 'æ‚²å‚·', 'å‚·å¿ƒ', 'å“­', 'ç—›è‹¦', 'æ²®å–ª', 'å¤±æœ›', 'éºæ†¾',
                    'å¯æ†', 'æ·š', 'çµ•æœ›', 'æ†‚é¬±', 'å­¤å–®', 'å¯‚å¯', 'ç„¡åŠ©', 'å¿ƒç—›',
                    'é›£å—', 'ä¸é–‹å¿ƒ', 'æ†‚å‚·', 'æ‚²æ…˜', 'æ…˜', 'æ·’æ…˜', 'æ‚½æ…˜',
                    'å‚·æ„Ÿ', 'æ„Ÿå‚·', 'å“€å‚·', 'æ‚²å“€', 'æ…˜æ·¡', 'ä½è½', 'æ¶ˆæ¥µ',
                    # è‹±æ–‡
                    'sad', 'unhappy', 'cry', 'tears', 'depressed', 'disappointed',
                    'miserable', 'sorry', 'painful', 'hurt', 'lonely', 'upset'
                ],
                'weight': 1.1
            },
            'neutral': {
                'keywords': [
                    # ä¸­æ–‡
                    'é‚„å¥½', 'æ™®é€š', 'ä¸€èˆ¬', 'å¹³å¸¸', 'æ²’ä»€éº¼', 'æ­£å¸¸', 'å¯ä»¥',
                    'å°šå¯', 'å¹³æ·¡', 'å¹³å‡¡', 'å¹³éœ', 'å†·éœ', 'ç†æ€§', 'å®¢è§€',
                    # è‹±æ–‡
                    'okay', 'fine', 'normal', 'neutral', 'average', 'so-so'
                ],
                'weight': 0.8
            }
        }
        
        # å¦å®šè©å’Œç¨‹åº¦å‰¯è©
        self.negation_words = ['ä¸', 'æ²’', 'ç„¡', 'é', 'æœª', 'åˆ¥', 'è«', 'å‹¿', 'æ¯‹']
        self.intensifiers = {
            'éå¸¸': 1.5, 'å¾ˆ': 1.3, 'è¶…': 1.4, 'ç‰¹åˆ¥': 1.3, 'æ¥µ': 1.5,
            'ç›¸ç•¶': 1.2, 'ååˆ†': 1.4, 'æ ¼å¤–': 1.3, 'å¤ª': 1.4, 'çœŸ': 1.2,
            'å¥½': 1.2, 'è¶…ç´š': 1.5, 'å·¨': 1.4, 'çˆ†': 1.4
        }
        
        # DeepFace åˆ°æˆ‘å€‘çš„æƒ…ç·’æ˜ å°„
        self.deepface_mapping = {
            'happy': 'happy',
            'angry': 'angry',
            'sad': 'sad',
            'neutral': 'neutral',
            'fear': 'sad',      # ææ‡¼æ­¸é¡ç‚ºæ‚²å‚·
            'disgust': 'angry',  # å­æƒ¡æ­¸é¡ç‚ºç”Ÿæ°£
            'surprise': 'happy'  # é©šè¨æ­¸é¡ç‚ºå¿«æ¨‚
        }
        
    def extract_audio(self):
        """æå–å½±ç‰‡éŸ³è¨Š"""
        print("ğŸ“½ï¸ æ­£åœ¨æå–éŸ³è¨Š...")
        try:
            video = VideoFileClip(self.video_path)
            audio_path = self.output_dir / "extracted_audio.wav"
            video.audio.write_audiofile(str(audio_path), verbose=False, logger=None)
            video.close()
            print(f"âœ… éŸ³è¨Šå·²ä¿å­˜åˆ°: {audio_path}")
            return str(audio_path)
        except Exception as e:
            print(f"âŒ éŸ³è¨Šæå–å¤±æ•—: {e}")
            return None
    
    def _map_deepface_to_basic_emotions(self, deepface_emotions):
        """å°‡ DeepFace çš„ 7 ç¨®æƒ…ç·’æ˜ å°„åˆ° 4 ç¨®åŸºæœ¬æƒ…ç·’"""
        basic_emotions = {emotion: 0.0 for emotion in self.emotion_labels}
        
        for deepface_emotion, score in deepface_emotions.items():
            basic_emotion = self.deepface_mapping.get(deepface_emotion.lower())
            if basic_emotion:
                basic_emotions[basic_emotion] += score
        
        # æ­£è¦åŒ–
        total = sum(basic_emotions.values())
        if total > 0:
            basic_emotions = {k: v/total for k, v in basic_emotions.items()}
        
        return basic_emotions
    
    def analyze_visual_emotions(self, sample_rate=1):
        """åˆ†æè¦–è¦ºæƒ…ç·’ï¼ˆè‡‰éƒ¨è¡¨æƒ…ï¼‰- åªè¼¸å‡º 4 ç¨®æƒ…ç·’"""
        print("\nğŸ‘¤ æ­£åœ¨åˆ†æè¦–è¦ºæƒ…ç·’...")
        
        cap = cv2.VideoCapture(self.video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = frame_count / fps
        
        frame_interval = int(fps * sample_rate)
        frame_idx = 0
        analyzed_count = 0
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_idx % frame_interval == 0:
                timestamp = frame_idx / fps
                
                try:
                    result = DeepFace.analyze(frame, actions=['emotion'], 
                                             enforce_detection=False, silent=True)
                    
                    if isinstance(result, list):
                        result = result[0]
                    
                    # å°‡ 7 ç¨®æƒ…ç·’æ˜ å°„åˆ° 4 ç¨®
                    deepface_emotions = result['emotion']
                    basic_emotions = self._map_deepface_to_basic_emotions(deepface_emotions)
                    
                    dominant_emotion = max(basic_emotions, key=basic_emotions.get)
                    
                    self.visual_emotions.append({
                        'timestamp': timestamp,
                        'dominant_emotion': dominant_emotion,
                        **basic_emotions
                    })
                    analyzed_count += 1
                    
                    if analyzed_count % 10 == 0:
                        print(f"  å·²åˆ†æ {analyzed_count} å¹€ ({timestamp:.1f}s / {duration:.1f}s)")
                
                except Exception as e:
                    self.visual_emotions.append({
                        'timestamp': timestamp,
                        'dominant_emotion': 'neutral',
                        **{emotion: 0 for emotion in self.emotion_labels}
                    })
            
            frame_idx += 1
        
        cap.release()
        print(f"âœ… è¦–è¦ºåˆ†æå®Œæˆï¼å…±åˆ†æ {analyzed_count} å¹€")
        
        # åŠ å…¥åˆ†ææ‘˜è¦å’ŒåŸå› è§£é‡‹
        if self.visual_emotions:
            self._explain_visual_analysis()
        
        return pd.DataFrame(self.visual_emotions)
    
    def _explain_visual_analysis(self):
        """è§£é‡‹è¦–è¦ºåˆ†æçµæœ"""
        df = pd.DataFrame(self.visual_emotions)
        
        print(f"\n   ğŸ“Š è¦–è¦ºæƒ…ç·’åˆ†ææ‘˜è¦ï¼š")
        emotion_zh = {'happy': 'å¿«æ¨‚', 'angry': 'ç”Ÿæ°£', 'sad': 'æ‚²å‚·', 'neutral': 'ä¸­æ€§'}
        
        # çµ±è¨ˆä¸»è¦æƒ…ç·’
        emotion_counts = df['dominant_emotion'].value_counts()
        total = len(df)
        
        print(f"   åœ¨ {total} å€‹æ™‚é–“é»ä¸­ï¼š")
        for emotion, count in emotion_counts.items():
            percentage = count / total * 100
            bar_length = int(percentage / 5)
            bar = 'â–ˆ' * bar_length + 'â–‘' * (20 - bar_length)
            print(f"   {emotion_zh[emotion]:3s}: {bar} {count:3d} æ¬¡ ({percentage:.1f}%)")
        
        # æ‰¾å‡ºæƒ…ç·’è®ŠåŒ–æœ€å¤§çš„æ™‚åˆ»
        print(f"\n   ğŸ’¡ åˆ†æè¦é»ï¼š")
        
        # 1. ä¸»è¦æƒ…ç·’
        dominant = emotion_counts.index[0]
        print(f"   1. æ•´é«”è¡¨æƒ…ä»¥ã€Œ{emotion_zh[dominant]}ã€ç‚ºä¸»")
        
        # 2. æƒ…ç·’è®ŠåŒ–
        emotion_changes = 0
        for i in range(1, len(df)):
            if df.iloc[i]['dominant_emotion'] != df.iloc[i-1]['dominant_emotion']:
                emotion_changes += 1
        
        if emotion_changes > len(df) * 0.3:
            print(f"   2. è¡¨æƒ…è®ŠåŒ–é »ç¹ï¼ˆå…± {emotion_changes} æ¬¡è®ŠåŒ–ï¼‰ï¼Œæƒ…ç·’è¼ƒä¸ç©©å®š")
        elif emotion_changes > 0:
            print(f"   3. è¡¨æƒ…æœ‰ {emotion_changes} æ¬¡è®ŠåŒ–ï¼Œæƒ…ç·’ç›¸å°ç©©å®š")
        else:
            print(f"   2. è¡¨æƒ…å§‹çµ‚ä¿æŒä¸€è‡´")
        
        # 3. æ‰¾å‡ºæœ€å¼·çƒˆçš„æƒ…ç·’æ™‚åˆ»
        for emotion in self.emotion_labels:
            if emotion in df.columns:
                max_idx = df[emotion].idxmax()
                max_value = df[emotion].max()
                if max_value > 0.7:  # åªé¡¯ç¤ºå¼·çƒˆçš„æƒ…ç·’
                    timestamp = df.iloc[max_idx]['timestamp']
                    print(f"   3. åœ¨ {timestamp:.1f} ç§’è™•ï¼Œã€Œ{emotion_zh[emotion]}ã€æƒ…ç·’æœ€å¼·çƒˆ ({max_value:.1%})")
                    break
    
    def analyze_audio_emotions(self, audio_path):
        """åˆ†æéŸ³è¨Šæƒ…ç·’ï¼ˆåŸºæ–¼è²å­¸ç‰¹å¾µï¼‰- åªè¼¸å‡º 4 ç¨®æƒ…ç·’"""
        print("\nğŸµ æ­£åœ¨åˆ†æéŸ³è¨Šæƒ…ç·’...")
        
        try:
            y, sr = librosa.load(audio_path, sr=None)
            duration = librosa.get_duration(y=y, sr=sr)
            
            segment_length = sr
            num_segments = int(duration)
            
            for i in range(num_segments):
                start_sample = i * segment_length
                end_sample = min((i + 1) * segment_length, len(y))
                segment = y[start_sample:end_sample]
                
                features = self._extract_audio_features(segment, sr)
                emotion_scores = self._map_features_to_emotions(features)
                
                self.audio_emotions.append({
                    'timestamp': i,
                    'dominant_emotion': max(emotion_scores, key=emotion_scores.get),
                    **emotion_scores
                })
            
            print(f"âœ… éŸ³è¨Šåˆ†æå®Œæˆï¼å…±åˆ†æ {num_segments} æ®µ")
            
            # åŠ å…¥åˆ†ææ‘˜è¦å’ŒåŸå› è§£é‡‹
            if self.audio_emotions:
                self._explain_audio_analysis()
            
            return pd.DataFrame(self.audio_emotions)
        
        except Exception as e:
            print(f"âŒ éŸ³è¨Šåˆ†æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def _explain_audio_analysis(self):
        """è§£é‡‹éŸ³è¨Šåˆ†æçµæœ"""
        df = pd.DataFrame(self.audio_emotions)
        
        print(f"\n   ğŸ“Š éŸ³è¨Šæƒ…ç·’åˆ†ææ‘˜è¦ï¼š")
        emotion_zh = {'happy': 'å¿«æ¨‚', 'angry': 'ç”Ÿæ°£', 'sad': 'æ‚²å‚·', 'neutral': 'ä¸­æ€§'}
        
        # çµ±è¨ˆä¸»è¦æƒ…ç·’
        emotion_counts = df['dominant_emotion'].value_counts()
        total = len(df)
        
        print(f"   åœ¨ {total} å€‹æ™‚é–“æ®µä¸­ï¼š")
        for emotion, count in emotion_counts.items():
            percentage = count / total * 100
            bar_length = int(percentage / 5)
            bar = 'â–ˆ' * bar_length + 'â–‘' * (20 - bar_length)
            print(f"   {emotion_zh[emotion]:3s}: {bar} {count:3d} æ¬¡ ({percentage:.1f}%)")
        
        print(f"\n   ğŸ’¡ åˆ†æè¦é»ï¼š")
        
        # 1. ä¸»è¦æƒ…ç·’
        dominant = emotion_counts.index[0]
        print(f"   1. è²éŸ³ç‰¹å¾µä¸»è¦é¡¯ç¤ºã€Œ{emotion_zh[dominant]}ã€æƒ…ç·’")
        
        # 2. éŸ³è¨Šç‰¹å¾µèªªæ˜
        reasoning = {
            'happy': "è²éŸ³èƒ½é‡è¼ƒé«˜ä¸”éŸ³èª¿æ˜äº®",
            'angry': "è²éŸ³èƒ½é‡é«˜ä¸”éŸ³èª¿è®ŠåŒ–å¤§",
            'sad': "è²éŸ³èƒ½é‡ä½ä¸”éŸ³èª¿è¼ƒä½æ²‰",
            'neutral': "è²éŸ³ç‰¹å¾µå¹³ç©©ï¼Œç„¡æ˜é¡¯æƒ…ç·’æ³¢å‹•"
        }
        print(f"   2. {reasoning[dominant]}")
        
        # 3. æƒ…ç·’è®ŠåŒ–
        emotion_changes = 0
        for i in range(1, len(df)):
            if df.iloc[i]['dominant_emotion'] != df.iloc[i-1]['dominant_emotion']:
                emotion_changes += 1
        
        if emotion_changes > 0:
            print(f"   3. èªèª¿æœ‰ {emotion_changes} æ¬¡æ˜é¡¯è®ŠåŒ–")
    
    def _extract_audio_features(self, audio_segment, sr):
        """æå–éŸ³è¨Šç‰¹å¾µ"""
        features = {}
        
        features['energy'] = np.mean(librosa.feature.rms(y=audio_segment))
        features['zcr'] = np.mean(librosa.feature.zero_crossing_rate(audio_segment))
        
        pitches, magnitudes = librosa.piptrack(y=audio_segment, sr=sr)
        pitch_values = []
        for t in range(pitches.shape[1]):
            index = magnitudes[:, t].argmax()
            pitch = pitches[index, t]
            if pitch > 0:
                pitch_values.append(pitch)
        features['pitch_mean'] = np.mean(pitch_values) if pitch_values else 0
        features['pitch_std'] = np.std(pitch_values) if pitch_values else 0
        
        mfcc = librosa.feature.mfcc(y=audio_segment, sr=sr, n_mfcc=13)
        features['mfcc_mean'] = np.mean(mfcc)
        features['mfcc_std'] = np.std(mfcc)
        
        spectral_centroids = librosa.feature.spectral_centroid(y=audio_segment, sr=sr)
        features['spectral_centroid'] = np.mean(spectral_centroids)
        
        return features
    
    def _map_features_to_emotions(self, features):
        """å°‡è²å­¸ç‰¹å¾µæ˜ å°„åˆ° 4 ç¨®åŸºæœ¬æƒ…ç·’"""
        emotions = {emotion: 0.0 for emotion in self.emotion_labels}
        
        # é«˜èƒ½é‡ + é«˜éŸ³èª¿ = happy
        if features['energy'] > 0.05 and features['pitch_mean'] > 200:
            emotions['happy'] = 0.6
        
        # é«˜èƒ½é‡ + é«˜è®ŠåŒ– = angry
        if features['energy'] > 0.06 and features['pitch_std'] > 50:
            emotions['angry'] = 0.7
        
        # ä½èƒ½é‡ + ä½éŸ³èª¿ = sad
        if features['energy'] < 0.03 and features['pitch_mean'] < 180:
            emotions['sad'] = 0.6
        
        # æ­£å¸¸åŒ–
        total = sum(emotions.values())
        if total == 0:
            emotions['neutral'] = 1.0
        else:
            emotions = {k: v/total for k, v in emotions.items()}
        
        return emotions
    
    def load_text_from_file(self):
        """å¾æª”æ¡ˆè¼‰å…¥æ–‡å­—ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰"""
        if not self.text_file:
            return None
        
        print(f"\nğŸ“„ æ­£åœ¨å¾æª”æ¡ˆè¼‰å…¥æ–‡å­—: {self.text_file}")
        try:
            with open(self.text_file, 'r', encoding='utf-8') as f:
                text = f.read().strip()
            print(f"âœ… æˆåŠŸè¼‰å…¥ {len(text)} å€‹å­—å…ƒ")
            return text
        except Exception as e:
            print(f"âŒ è¼‰å…¥æ–‡å­—æª”æ¡ˆå¤±æ•—: {e}")
            return None
    
    
    def _explain_emotion_reasoning(self, text, dominant_emotion, basic_emotions, emotion_details):
        """è§£é‡‹æƒ…ç·’åˆ¤æ–·çš„åŸå› """
        
        # æ‰¾å‡ºæ–‡å­—ä¸­çš„é—œéµè©ï¼ˆä½¿ç”¨é¡åˆ¥å±¬æ€§ï¼‰
        found_keywords = {emotion: [] for emotion in self.emotion_labels}
        text_lower = text.lower()
        
        for emotion, config in self.emotion_keywords.items():
            for keyword in config['keywords']:
                if keyword.lower() in text_lower:
                    found_keywords[emotion].append(keyword)
        
        # é¡¯ç¤ºä¸»è¦æƒ…ç·’çš„åŸå› 
        emotion_zh = {'happy': 'å¿«æ¨‚', 'angry': 'ç”Ÿæ°£', 'sad': 'æ‚²å‚·', 'neutral': 'ä¸­æ€§'}
        
        reasons = []
        
        # åŸå›  1: é—œéµè©
        if found_keywords[dominant_emotion]:
            keywords_str = 'ã€'.join(found_keywords[dominant_emotion][:5])  # æœ€å¤šé¡¯ç¤º 5 å€‹
            reasons.append(f"æª¢æ¸¬åˆ°ç›¸é—œè©å½™ï¼š{keywords_str}")
        
        # åŸå›  2: åˆ†æ•¸å·®è·
        sorted_emotions = sorted(basic_emotions.items(), key=lambda x: x[1], reverse=True)
        if len(sorted_emotions) > 1:
            second_emotion, second_score = sorted_emotions[1]
            dominant_score = basic_emotions[dominant_emotion]
            
            if dominant_score - second_score > 0.3:
                reasons.append(f"æƒ…ç·’å¼·åº¦æ˜é¡¯é«˜æ–¼å…¶ä»–æƒ…ç·’ï¼ˆé«˜å‡º {(dominant_score-second_score):.1%}ï¼‰")
            elif dominant_score - second_score < 0.1:
                reasons.append(f"èˆ‡ã€Œ{emotion_zh[second_emotion]}ã€æƒ…ç·’ç›¸è¿‘ï¼Œä½†ç•¥é«˜ä¸€äº›")
        
        # åŸå›  3: åŸå§‹æ¨¡å‹çš„æƒ…ç·’åˆ†ä½ˆï¼ˆåƒ…ç”¨æ–¼è‹±æ–‡æ¨¡å‹ï¼‰
        if emotion_details and self.use_chinese_model == False:
            print(f"   åŸå§‹æ¨¡å‹æª¢æ¸¬åˆ°çš„æƒ…ç·’ï¼š")
            for orig_emotion, scores in emotion_details.items():
                avg_score = sum(scores) / len(scores) if isinstance(scores, list) else scores
                if avg_score > 0.1:
                    print(f"      â€¢ {orig_emotion}: {avg_score:.1%}")
        
        # åŸå›  4: æƒ…ç·’æ··åˆ
        high_emotions = [e for e, s in basic_emotions.items() if s > 0.2]
        if len(high_emotions) > 1:
            emotions_str = 'ã€'.join([emotion_zh[e] for e in high_emotions])
            reasons.append(f"æ–‡å­—åŒ…å«å¤šç¨®æƒ…ç·’ï¼š{emotions_str}")
        
        # åŸå›  5: æ–‡å­—é•·åº¦
        if len(text) > 500:
            reasons.append(f"æ–‡å­—è¼ƒé•·ï¼ŒåŒ…å«å¤šå€‹æƒ…ç·’æ®µè½")
        
        # é¡¯ç¤ºåŸå› 
        if reasons:
            for i, reason in enumerate(reasons, 1):
                print(f"      {i}. {reason}")
        else:
            print(f"      æ ¹æ“šæ•´é«”èªå¢ƒåˆ¤æ–·ç‚ºã€Œ{emotion_zh[dominant_emotion]}ã€")
        
        # å¦‚æœæœ‰å…¶ä»–æ˜é¡¯æƒ…ç·’ï¼Œä¹Ÿèªªæ˜
        other_emotions = [(e, s) for e, s in basic_emotions.items() 
                         if e != dominant_emotion and s > 0.15]
        if other_emotions:
            print(f"\n   âš ï¸ åŒæ™‚æª¢æ¸¬åˆ°å…¶ä»–æƒ…ç·’ï¼š")
            for emotion, score in sorted(other_emotions, key=lambda x: x[1], reverse=True):
                keywords = found_keywords[emotion]
                if keywords:
                    keywords_str = 'ã€'.join(keywords[:3])
                    print(f"      â€¢ {emotion_zh[emotion]} ({score:.1%})ï¼šå¯èƒ½å› ç‚ºã€Œ{keywords_str}ã€ç­‰è©å½™")
                else:
                    print(f"      â€¢ {emotion_zh[emotion]} ({score:.1%})ï¼šæ ¹æ“šæ•´é«”èªå¢ƒ")
    
    def transcribe_and_analyze_text(self, audio_path):
        """è½‰éŒ„èªéŸ³ä¸¦åˆ†ææ–‡å­—æƒ…ç·’ï¼ˆæ”¹é€²ç‰ˆï¼‰"""
        
        # å„ªå…ˆä½¿ç”¨è‡ªè¨‚æ–‡å­—æª”æ¡ˆ
        if self.text_file:
            text = self.load_text_from_file()
            if text:
                return self._analyze_text_emotion(text)
        
        # å¦‚æœæ²’æœ‰è‡ªè¨‚æ–‡å­—ï¼Œæ‰ä½¿ç”¨èªéŸ³è½‰éŒ„
        print("\nğŸ“ æ­£åœ¨è½‰éŒ„èªéŸ³...")
        
        try:
            recognizer = sr.Recognizer()
            
            with sr.AudioFile(audio_path) as source:
                audio_data = recognizer.record(source)
            
            text_zh = ""
            text_en = ""
            
            try:
                text_zh = recognizer.recognize_google(audio_data, language='zh-TW')
                print(f"  ä¸­æ–‡è½‰éŒ„: {text_zh}")
            except:
                print("  ç„¡æ³•è¾¨è­˜ä¸­æ–‡èªéŸ³")
            
            try:
                text_en = recognizer.recognize_google(audio_data, language='en-US')
                print(f"  è‹±æ–‡è½‰éŒ„: {text_en}")
            except:
                print("  ç„¡æ³•è¾¨è­˜è‹±æ–‡èªéŸ³")
            
            text = text_zh if text_zh else text_en
            
            if text:
                return self._analyze_text_emotion(text)
            else:
                print("âš ï¸ ç„¡æ³•è½‰éŒ„èªéŸ³")
                return None, None
        
        except Exception as e:
            print(f"âŒ èªéŸ³è½‰éŒ„å¤±æ•—: {e}")
            return None, None
    
    def _analyze_text_emotion(self, text):
        """åˆ†ææ–‡å­—æƒ…ç·’ï¼ˆå…§éƒ¨æ–¹æ³•ï¼‰- æ”¯æ´ä¸­è‹±æ–‡æ¨¡å‹"""
        print("\nğŸ’¬ æ­£åœ¨åˆ†ææ–‡å­—æƒ…ç·’...")
        
        try:
            # è™•ç†é•·æ–‡å­—ï¼šå¦‚æœå¤ªé•·ï¼Œåˆ†æ®µåˆ†æ
            max_length = 400  # å®‰å…¨é•·åº¦ï¼Œç¢ºä¿ä¸è¶…é 512 tokens
            
            if len(text) > max_length:
                print(f"   âš ï¸ æ–‡å­—è¼ƒé•· ({len(text)} å­—å…ƒ)ï¼Œå°‡åˆ†æ®µåˆ†æ...")
                # åˆ†æˆå¤šæ®µ
                segments = []
                for i in range(0, len(text), max_length):
                    segments.append(text[i:i+max_length])
                print(f"   åˆ†ç‚º {len(segments)} æ®µé€²è¡Œåˆ†æ")
            else:
                segments = [text]
            
            # é¸æ“‡æ¨¡å‹
            if self.use_chinese_model:
                print("   ä½¿ç”¨ä¸­æ–‡æƒ…ç·’åˆ†ææ¨¡å‹...")
                emotion_scores = self._analyze_chinese_emotion(segments)
            else:
                print("   ä½¿ç”¨è‹±æ–‡æƒ…ç·’åˆ†ææ¨¡å‹...")
                emotion_scores = self._analyze_english_emotion(segments)
            
            # æ‰¾å‡ºä¸»è¦æƒ…ç·’
            dominant_emotion = max(emotion_scores, key=emotion_scores.get)
            
            # å„²å­˜è©³ç´°è³‡è¨Šï¼ˆç”¨æ–¼è§£é‡‹ï¼‰
            self.text_emotions.append({
                'text': text,
                'dominant_emotion': dominant_emotion,
                'emotion_details': emotion_scores,
                **emotion_scores
            })
            
            print(f"âœ… æ–‡å­—åˆ†æå®Œæˆï¼")
            print(f"   æ–‡å­—å…§å®¹: {text[:100]}..." if len(text) > 100 else f"   æ–‡å­—å…§å®¹: {text}")
            
            # è©³ç´°æƒ…ç·’åˆ†æèªªæ˜
            print(f"\n   ğŸ“Š æƒ…ç·’åˆ†æçµæœï¼š")
            emotion_zh = {'happy': 'å¿«æ¨‚', 'angry': 'ç”Ÿæ°£', 'sad': 'æ‚²å‚·', 'neutral': 'ä¸­æ€§'}
            
            # æ’åºé¡¯ç¤ºï¼ˆç”±é«˜åˆ°ä½ï¼‰
            sorted_emotions = sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True)
            for emotion, score in sorted_emotions:
                bar_length = int(score * 20)  # 20 å€‹å­—å…ƒçš„é€²åº¦æ¢
                bar = 'â–ˆ' * bar_length + 'â–‘' * (20 - bar_length)
                print(f"   {emotion_zh[emotion]:3s} ({emotion:7s}): {bar} {score:.1%}")
            
            # è§£é‡‹ç‚ºä»€éº¼æ˜¯é€™å€‹æƒ…ç·’
            print(f"\n   ğŸ’¡ ç‚ºä»€éº¼æ˜¯ã€Œ{emotion_zh[dominant_emotion]}ã€ï¼Ÿ")
            self._explain_emotion_reasoning(text, dominant_emotion, emotion_scores, {})
            
            return text, emotion_scores
        
        except Exception as e:
            print(f"âŒ æ–‡å­—æƒ…ç·’åˆ†æå¤±æ•—: {e}")
            return None, None
    
    def _analyze_chinese_emotion(self, segments):
        """ä½¿ç”¨ä¸­æ–‡æ¨¡å‹åˆ†ææƒ…ç·’"""
        try:
            # æ–¹æ³• 1: å˜—è©¦ä½¿ç”¨ ckiplab çš„ä¸­æ–‡ BERT æƒ…ç·’æ¨¡å‹
            try:
                from transformers import BertTokenizer, BertForSequenceClassification
                import torch
                
                print("   è¼‰å…¥ ckiplab ä¸­æ–‡æƒ…ç·’æ¨¡å‹...")
                # é€™å€‹æ¨¡å‹å°ˆé–€ç‚ºä¸­æ–‡æƒ…ç·’åˆ†æè¨“ç·´
                model_name = "ckiplab/bert-base-chinese-ner"
                tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
                
                # ä½¿ç”¨é—œéµè©æ–¹æ³•ä½œç‚ºä¸»è¦æ–¹å¼
                emotion_scores = self._keyword_based_emotion_analysis(segments)
                print("   ä½¿ç”¨é—œéµè©æ–¹æ³•åˆ†æ")
                
                return emotion_scores
                
            except Exception as e:
                print(f"   âš ï¸ ä¸­æ–‡ BERT æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é—œéµè©æ–¹æ³•")
                return self._keyword_based_emotion_analysis(segments)
        
        except Exception as e:
            print(f"   âš ï¸ åˆ†æå¤±æ•—: {e}ï¼Œä½¿ç”¨é—œéµè©æ–¹æ³•")
            return self._keyword_based_emotion_analysis(segments)
    
    def _keyword_based_emotion_analysis(self, segments):
        """åŸºæ–¼é—œéµè©çš„æƒ…ç·’åˆ†æï¼ˆä½¿ç”¨é¡åˆ¥å±¬æ€§çš„é—œéµè©å­—å…¸ï¼‰"""
        
        emotion_scores = {emotion: 0.0 for emotion in self.emotion_labels}
        
        # åˆ†ææ¯å€‹æ®µè½
        for segment in segments:
            segment_lower = segment.lower()
            
            # æª¢æŸ¥æ¯å€‹æƒ…ç·’çš„é—œéµè©
            for emotion, config in self.emotion_keywords.items():
                for keyword in config['keywords']:
                    if keyword in segment:
                        score = config['weight']
                        
                        # æª¢æŸ¥å‰é¢æ˜¯å¦æœ‰å¦å®šè©ï¼ˆå¾€å‰çœ‹ 2 å€‹å­—ï¼‰
                        keyword_pos = segment.find(keyword)
                        start_idx = max(0, keyword_pos - 2)
                        prefix = segment[start_idx:keyword_pos]
                        
                        has_negation = any(neg in prefix for neg in self.negation_words)
                        
                        # æª¢æŸ¥ç¨‹åº¦å‰¯è©
                        intensifier_multiplier = 1.0
                        for intensifier, multiplier in self.intensifiers.items():
                            if intensifier in prefix:
                                intensifier_multiplier = multiplier
                                break
                        
                        # è¨ˆç®—åˆ†æ•¸
                        if has_negation:
                            # å¦å®šè©æœƒé™ä½é€™å€‹æƒ…ç·’ï¼Œå¢åŠ  neutral
                            score *= -0.5
                            emotion_scores['neutral'] += 0.3
                        else:
                            score *= intensifier_multiplier
                        
                        emotion_scores[emotion] += score
        
        # æ­£è¦åŒ–åˆ†æ•¸
        total = sum(emotion_scores.values())
        if total > 0:
            emotion_scores = {k: v/total for k, v in emotion_scores.items()}
        else:
            # å¦‚æœæ²’æœ‰æª¢æ¸¬åˆ°ä»»ä½•æƒ…ç·’ï¼Œè¨­ç‚º neutral
            emotion_scores['neutral'] = 1.0
        
        return emotion_scores
    
    def _analyze_english_emotion(self, segments):
        """ä½¿ç”¨è‹±æ–‡æ¨¡å‹åˆ†ææƒ…ç·’ï¼ˆåŸæœ¬çš„æ–¹æ³•ï¼‰"""
        emotion_analyzer = pipeline("text-classification", 
                                   model="j-hartmann/emotion-english-distilroberta-base",
                                   top_k=None,
                                   truncation=True,
                                   max_length=512)
        
        # åˆ†ææ¯ä¸€æ®µä¸¦å–å¹³å‡
        all_emotions = []
        for idx, segment in enumerate(segments):
            if len(segments) > 1:
                print(f"   åˆ†æç¬¬ {idx+1}/{len(segments)} æ®µ...")
            results = emotion_analyzer(segment)
            all_emotions.append(results[0])
        
        # åˆä½µå¤šæ®µçµæœï¼ˆå–å¹³å‡ï¼‰
        if len(all_emotions) > 1:
            print(f"   åˆä½µ {len(all_emotions)} æ®µçš„åˆ†æçµæœ...")
        
        results = [all_emotions[0]]  # ä½¿ç”¨ç¬¬ä¸€æ®µçš„çµæœçµæ§‹
        # å¹³å‡æ‰€æœ‰æ®µçš„åˆ†æ•¸
        emotion_scores_sum = {}
        for segment_results in all_emotions:
            for item in segment_results:
                label = item['label']
                score = item['score']
                emotion_scores_sum[label] = emotion_scores_sum.get(label, 0) + score
        
        # å–å¹³å‡ä¸¦é‡å»ºçµæœæ ¼å¼
        results = [[{'label': label, 'score': score / len(all_emotions)} 
                   for label, score in emotion_scores_sum.items()]]
        
        # æ˜ å°„åˆ° 4 ç¨®åŸºæœ¬æƒ…ç·’
        emotion_mapping = {
            'joy': 'happy',
            'happiness': 'happy',
            'anger': 'angry',
            'sadness': 'sad',
            'fear': 'sad',
            'disgust': 'angry',
            'surprise': 'happy',
            'neutral': 'neutral'
        }
        
        basic_emotions = {emotion: 0.0 for emotion in self.emotion_labels}
        
        for result in results[0]:
            label = result['label'].lower()
            score = result['score']
            mapped_emotion = emotion_mapping.get(label, 'neutral')
            basic_emotions[mapped_emotion] += score
        
        # æ­£è¦åŒ–
        total = sum(basic_emotions.values())
        if total > 0:
            basic_emotions = {k: v/total for k, v in basic_emotions.items()}
        
        return basic_emotions
    
    def calculate_correlations(self, visual_df, audio_df):
        """è¨ˆç®—ä¸åŒæ¨¡æ…‹ä¹‹é–“çš„ç›¸é—œæ€§
        
        èªªæ˜ï¼š
        - å°æ¯å€‹æƒ…ç·’ï¼Œè¨ˆç®—è¦–è¦ºå’ŒéŸ³è¨Šåœ¨æ™‚é–“åºåˆ—ä¸Šçš„çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸
        - ç›¸é—œä¿‚æ•¸ç¯„åœï¼š-1 åˆ° 1
        - æ¥è¿‘ 1ï¼šå…©è€…è®ŠåŒ–è¶¨å‹¢éå¸¸ä¸€è‡´ï¼ˆåŒæ™‚é«˜ã€åŒæ™‚ä½ï¼‰
        - æ¥è¿‘ 0ï¼šå…©è€…è®ŠåŒ–ç„¡é—œ
        - æ¥è¿‘ -1ï¼šå…©è€…è®ŠåŒ–è¶¨å‹¢ç›¸å
        """
        print("\nğŸ“Š æ­£åœ¨è¨ˆç®—ç›¸é—œæ€§...")
        print("   èªªæ˜ï¼šè¨ˆç®—è¦–è¦ºå’ŒéŸ³è¨Šæƒ…ç·’å¼·åº¦çš„æ™‚é–“åºåˆ—ç›¸é—œæ€§")
        
        correlations = {}
        
        # å°é½Šæ™‚é–“æˆ³
        common_timestamps = set(visual_df['timestamp']).intersection(set(audio_df['timestamp']))
        
        if len(common_timestamps) > 0:
            visual_aligned = visual_df[visual_df['timestamp'].isin(common_timestamps)].sort_values('timestamp')
            audio_aligned = audio_df[audio_df['timestamp'].isin(common_timestamps)].sort_values('timestamp')
            
            for emotion in self.emotion_labels:
                if emotion in visual_aligned.columns and emotion in audio_aligned.columns:
                    visual_values = visual_aligned[emotion].values
                    audio_values = audio_aligned[emotion].values
                    
                    # è¨ˆç®—çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸
                    corr = np.corrcoef(visual_values, audio_values)[0, 1]
                    correlations[emotion] = corr if not np.isnan(corr) else 0
                    
                    # è©³ç´°èªªæ˜
                    print(f"   {emotion}: {corr:.3f}", end="")
                    if corr > 0.5:
                        print(" (é«˜åº¦ä¸€è‡´ âœ…)")
                    elif corr > 0.3:
                        print(" (ä¸­åº¦ä¸€è‡´ âš ï¸)")
                    else:
                        print(" (ä¸€è‡´æ€§è¼ƒä½ âŒ)")
            
            print("âœ… ç›¸é—œæ€§è¨ˆç®—å®Œæˆï¼")
        else:
            print("âš ï¸ ç„¡æ³•å°é½Šæ™‚é–“æˆ³")
        
        return correlations
    
    def visualize_results(self, visual_df, audio_df, text_emotion=None, correlations=None):
        """è¦–è¦ºåŒ–åˆ†æçµæœï¼ˆæ”¹é€²ç‰ˆï¼šåªé¡¯ç¤º 4 ç¨®æƒ…ç·’ï¼‰"""
        print("\nğŸ“ˆ æ­£åœ¨ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...")
        
        plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS', 'SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig = plt.figure(figsize=(20, 12))
        gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)
        
        # å®šç¾©ä¸­æ–‡æ¨™ç±¤
        emotion_labels_zh = {
            'happy': 'å¿«æ¨‚',
            'angry': 'ç”Ÿæ°£',
            'sad': 'æ‚²å‚·',
            'neutral': 'ä¸­æ€§'
        }
        
        colors = {
            'happy': '#FFD700',    # é‡‘é»ƒè‰²
            'angry': '#FF4444',    # ç´…è‰²
            'sad': '#4169E1',      # è—è‰²
            'neutral': '#808080'   # ç°è‰²
        }
        
        # 1. è¦–è¦ºæƒ…ç·’æ™‚é–“åºåˆ—
        ax1 = fig.add_subplot(gs[0, :])
        for emotion in self.emotion_labels:
            if emotion in visual_df.columns:
                ax1.plot(visual_df['timestamp'], visual_df[emotion], 
                        label=f'{emotion_labels_zh[emotion]} ({emotion})', 
                        marker='o', markersize=4, linewidth=2.5,
                        color=colors[emotion])
        ax1.set_xlabel('æ™‚é–“ (ç§’)', fontsize=12, fontweight='bold')
        ax1.set_ylabel('æƒ…ç·’å¼·åº¦', fontsize=12, fontweight='bold')
        ax1.set_title('è¦–è¦ºæƒ…ç·’åˆ†æ (Visual/Facial Emotion)', fontsize=14, fontweight='bold')
        ax1.legend(loc='upper right', ncol=4, fontsize=10)
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(-0.05, 1.05)
        
        # 2. éŸ³è¨Šæƒ…ç·’æ™‚é–“åºåˆ—
        ax2 = fig.add_subplot(gs[1, :])
        for emotion in self.emotion_labels:
            if emotion in audio_df.columns:
                ax2.plot(audio_df['timestamp'], audio_df[emotion], 
                        label=f'{emotion_labels_zh[emotion]} ({emotion})', 
                        marker='s', markersize=4, linewidth=2.5,
                        color=colors[emotion])
        ax2.set_xlabel('æ™‚é–“ (ç§’)', fontsize=12, fontweight='bold')
        ax2.set_ylabel('æƒ…ç·’å¼·åº¦', fontsize=12, fontweight='bold')
        ax2.set_title('éŸ³è¨Šæƒ…ç·’åˆ†æ (Audio/Voice Emotion)', fontsize=14, fontweight='bold')
        ax2.legend(loc='upper right', ncol=4, fontsize=10)
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(-0.05, 1.05)
        
        # 3. ç–ŠåŠ æ¯”è¼ƒåœ–
        ax3 = fig.add_subplot(gs[2, :])
        for emotion in self.emotion_labels:
            if emotion in visual_df.columns:
                ax3.plot(visual_df['timestamp'], visual_df[emotion], 
                        label=f'{emotion_labels_zh[emotion]} (è¦–è¦º)', 
                        linestyle='-', linewidth=2.5, color=colors[emotion], alpha=0.8)
            if emotion in audio_df.columns:
                ax3.plot(audio_df['timestamp'], audio_df[emotion], 
                        label=f'{emotion_labels_zh[emotion]} (éŸ³è¨Š)', 
                        linestyle='--', linewidth=2.5, color=colors[emotion], alpha=0.6)
        
        ax3.set_xlabel('æ™‚é–“ (ç§’)', fontsize=12, fontweight='bold')
        ax3.set_ylabel('æƒ…ç·’å¼·åº¦', fontsize=12, fontweight='bold')
        ax3.set_title('è¦–è¦º vs éŸ³è¨Šæƒ…ç·’æ¯”è¼ƒ (Visual vs Audio Comparison)', 
                     fontsize=14, fontweight='bold')
        ax3.legend(loc='upper right', ncol=4, fontsize=9)
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(-0.05, 1.05)
        
        # 4. ä¸»è¦æƒ…ç·’åˆ†å¸ƒ - è¦–è¦º
        ax4 = fig.add_subplot(gs[3, 0])
        visual_counts = visual_df['dominant_emotion'].value_counts()
        pie_colors = [colors[emotion] for emotion in visual_counts.index]
        pie_labels = [f"{emotion_labels_zh[e]}\n({e})" for e in visual_counts.index]
        ax4.pie(visual_counts.values, labels=pie_labels, autopct='%1.1f%%',
               colors=pie_colors, startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})
        ax4.set_title('è¦–è¦ºä¸»è¦æƒ…ç·’åˆ†å¸ƒ', fontsize=12, fontweight='bold')
        
        # 5. ä¸»è¦æƒ…ç·’åˆ†å¸ƒ - éŸ³è¨Š
        ax5 = fig.add_subplot(gs[3, 1])
        audio_counts = audio_df['dominant_emotion'].value_counts()
        pie_colors = [colors[emotion] for emotion in audio_counts.index]
        pie_labels = [f"{emotion_labels_zh[e]}\n({e})" for e in audio_counts.index]
        ax5.pie(audio_counts.values, labels=pie_labels, autopct='%1.1f%%',
               colors=pie_colors, startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})
        ax5.set_title('éŸ³è¨Šä¸»è¦æƒ…ç·’åˆ†å¸ƒ', fontsize=12, fontweight='bold')
        
        # 6. ç›¸é—œæ€§ç†±åœ–
        if correlations:
            ax6 = fig.add_subplot(gs[3, 2])
            # è½‰æ›ç‚ºä¸­æ–‡æ¨™ç±¤
            corr_data_zh = {emotion_labels_zh[k]: v for k, v in correlations.items()}
            corr_df = pd.DataFrame([corr_data_zh])
            sns.heatmap(corr_df, annot=True, fmt='.3f', cmap='RdYlGn', 
                       center=0, vmin=-1, vmax=1, ax=ax6, 
                       cbar_kws={'label': 'ç›¸é—œä¿‚æ•¸'})
            ax6.set_title('è¦–è¦º-éŸ³è¨Šç›¸é—œæ€§\n(Visual-Audio Correlation)', 
                         fontsize=12, fontweight='bold')
            ax6.set_yticklabels([])
        
        plt.suptitle('å¤šæ¨¡æ…‹æƒ…ç·’åˆ†æçµæœ (Multi-Modal Emotion Analysis)\nå–œæ€’å“€æ¨‚å››ç¨®åŸºæœ¬æƒ…ç·’', 
                    fontsize=16, fontweight='bold', y=0.995)
        
        output_path = self.output_dir / "emotion_analysis_visualization.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ… åœ–è¡¨å·²ä¿å­˜åˆ°: {output_path}")
        
        return fig
    
    def save_results(self, visual_df, audio_df, text_emotion, correlations):
        """ä¿å­˜åˆ†æçµæœ"""
        print("\nğŸ’¾ æ­£åœ¨ä¿å­˜çµæœ...")
        
        visual_df.to_csv(self.output_dir / "visual_emotions.csv", index=False, encoding='utf-8-sig')
        audio_df.to_csv(self.output_dir / "audio_emotions.csv", index=False, encoding='utf-8-sig')
        
        summary = {
            'video_path': str(self.video_path),
            'text_file': str(self.text_file) if self.text_file else None,
            'emotion_labels': self.emotion_labels,
            'visual_emotion_stats': visual_df['dominant_emotion'].value_counts().to_dict(),
            'audio_emotion_stats': audio_df['dominant_emotion'].value_counts().to_dict(),
            'correlations': correlations if correlations else {},
            'text_emotion': text_emotion if text_emotion else {}
        }
        
        with open(self.output_dir / "analysis_summary.json", 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… çµæœå·²ä¿å­˜åˆ°: {self.output_dir}")
    
    def run_full_analysis(self, sample_rate=1):
        """åŸ·è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        print("=" * 80)
        print("ğŸ¬ é–‹å§‹å¤šæ¨¡æ…‹æƒ…ç·’åˆ†æï¼ˆæ”¹é€²ç‰ˆï¼‰")
        print("   - æƒ…ç·’é¡åˆ¥ï¼šå–œæ€’å“€æ¨‚å››ç¨®åŸºæœ¬æƒ…ç·’")
        if self.text_file:
            print(f"   - ä½¿ç”¨è‡ªè¨‚æ–‡å­—æª”æ¡ˆï¼š{self.text_file}")
        print("=" * 80)
        
        # 1. æå–éŸ³è¨Š
        audio_path = self.extract_audio()
        
        # 2. è¦–è¦ºåˆ†æ
        visual_df = self.analyze_visual_emotions(sample_rate=sample_rate)
        
        # 3. éŸ³è¨Šåˆ†æ
        audio_df = pd.DataFrame()
        if audio_path:
            audio_df = self.analyze_audio_emotions(audio_path)
        
        # 4. æ–‡å­—åˆ†æï¼ˆå„ªå…ˆä½¿ç”¨è‡ªè¨‚æ–‡å­—ï¼‰
        text_emotion = None
        if audio_path or self.text_file:
            text, text_emotion = self.transcribe_and_analyze_text(audio_path)
        
        # 5. è¨ˆç®—ç›¸é—œæ€§
        correlations = None
        if not visual_df.empty and not audio_df.empty:
            correlations = self.calculate_correlations(visual_df, audio_df)
        
        # 6. è¦–è¦ºåŒ–
        if not visual_df.empty and not audio_df.empty:
            self.visualize_results(visual_df, audio_df, text_emotion, correlations)
        
        # 7. ä¿å­˜çµæœ
        self.save_results(visual_df, audio_df, text_emotion, correlations)
        
        # 8. æ‰“å°æ‘˜è¦
        self.print_summary(visual_df, audio_df, correlations)
        
        print("\n" + "=" * 80)
        print("âœ… åˆ†æå®Œæˆï¼")
        print("=" * 80)
    
    def print_summary(self, visual_df, audio_df, correlations):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ“‹ åˆ†ææ‘˜è¦ï¼ˆå–œæ€’å“€æ¨‚å››ç¨®æƒ…ç·’ï¼‰")
        print("=" * 80)
        
        emotion_zh = {'happy': 'å¿«æ¨‚', 'angry': 'ç”Ÿæ°£', 'sad': 'æ‚²å‚·', 'neutral': 'ä¸­æ€§'}
        
        if not visual_df.empty:
            print("\nè¦–è¦ºæƒ…ç·’çµ±è¨ˆ:")
            for emotion, count in visual_df['dominant_emotion'].value_counts().items():
                print(f"  {emotion_zh[emotion]} ({emotion}): {count} æ¬¡")
            print(f"\næœ€å¸¸è¦‹çš„è¦–è¦ºæƒ…ç·’: {emotion_zh[visual_df['dominant_emotion'].mode()[0]]} ({visual_df['dominant_emotion'].mode()[0]})")
        
        if not audio_df.empty:
            print("\néŸ³è¨Šæƒ…ç·’çµ±è¨ˆ:")
            for emotion, count in audio_df['dominant_emotion'].value_counts().items():
                print(f"  {emotion_zh[emotion]} ({emotion}): {count} æ¬¡")
            print(f"\næœ€å¸¸è¦‹çš„éŸ³è¨Šæƒ…ç·’: {emotion_zh[audio_df['dominant_emotion'].mode()[0]]} ({audio_df['dominant_emotion'].mode()[0]})")
        
        if correlations:
            print("\nè¦–è¦º-éŸ³è¨Šç›¸é—œæ€§:")
            for emotion, corr in sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True):
                status = "âœ…" if corr > 0.5 else ("âš ï¸" if corr > 0.3 else "âŒ")
                print(f"  {emotion_zh[emotion]} ({emotion}): {corr:.3f} {status}")
            
            avg_corr = np.mean(list(correlations.values()))
            print(f"\nå¹³å‡ç›¸é—œä¿‚æ•¸: {avg_corr:.3f}")
            
            if avg_corr > 0.5:
                print("âœ… è¦–è¦ºå’ŒéŸ³è¨Šæƒ…ç·’é«˜åº¦ä¸€è‡´")
            elif avg_corr > 0.3:
                print("âš ï¸ è¦–è¦ºå’ŒéŸ³è¨Šæƒ…ç·’ä¸­åº¦ä¸€è‡´")
            else:
                print("âŒ è¦–è¦ºå’ŒéŸ³è¨Šæƒ…ç·’ä¸€è‡´æ€§è¼ƒä½")


def main():
    """ä¸»ç¨‹å¼"""
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python multimodal_emotion_analysis.py <video_path> [text_file] [sample_rate]")
        print("\nç¯„ä¾‹:")
        print("  python multimodal_emotion_analysis.py my_vlog.mp4")
        print("  python multimodal_emotion_analysis.py my_vlog.mp4 transcript.txt")
        print("  python multimodal_emotion_analysis.py my_vlog.mp4 transcript.txt 2")
        print("\nåƒæ•¸èªªæ˜:")
        print("  video_path: å½±ç‰‡è·¯å¾‘ï¼ˆå¿…å¡«ï¼‰")
        print("  text_file: æ–‡å­—æª”æ¡ˆè·¯å¾‘ï¼ˆé¸å¡«ï¼Œå¦‚æœæœ‰çš„è©±æœƒç›´æ¥ä½¿ç”¨è€Œä¸é€²è¡ŒèªéŸ³è½‰éŒ„ï¼‰")
        print("  sample_rate: æ¯Nç§’åˆ†æä¸€å¹€ï¼ˆé¸å¡«ï¼Œé è¨­=1ï¼‰")
        sys.exit(1)
    
    video_path = sys.argv[1]
    text_file = sys.argv[2] if len(sys.argv) > 2 and sys.argv[2].endswith('.txt') else None
    sample_rate = float(sys.argv[-1]) if len(sys.argv) > 2 and not sys.argv[-1].endswith('.txt') else 1.0
    
    if not Path(video_path).exists():
        print(f"âŒ æ‰¾ä¸åˆ°å½±ç‰‡æª”æ¡ˆ: {video_path}")
        sys.exit(1)
    
    if text_file and not Path(text_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡å­—æª”æ¡ˆ: {text_file}")
        sys.exit(1)
    
    # åŸ·è¡Œåˆ†æ
    analyzer = MultiModalEmotionAnalyzer(video_path, text_file=text_file)
    analyzer.run_full_analysis(sample_rate=sample_rate)


if __name__ == "__main__":
    main()